# Market-Basket-Analysis
Market basket analysis is known as a data mining technique that analyses the patterns of co-occurrence of certain products and investigates the relationship and strength of the link between the products that have been purchased together by the customers. The main objective of market basket analysis is to identify the purchasing behavior of the customers in order to help retailers to understand better which are the popular products and try to get more stocks of those certain items in the inventory. In addition, retailers could find out the best strategic location to display their products by placing the products that are being bought together frequently in the same area. By having a more effective product placement, retailers could boost their sales by a significant amount. 

## Overview
The project will focus on the analysis of association rules in different time periods using market basket analysis. Market basket analysis will be conducted using a dataset of customer transactions which are divided into different time periods such as quarterly, monthly, and biweekly. First of all, the data collected from the retail store will undergo a data preprocessing process which involves data cleaning and preparation of the data for analysis later on. In this phase, the data will be checked for completeness and consistency to guarantee quality of data. Any missing values will be input, and the outliers will be removed. Subsequently, market basket analysis will be performed using the prepared data to discover association rules in different time periods. The discovered association rules will then undergo a filtering process which will disqualify those that do not meet the minimum requirement in terms of the value of the important parameters in market basket analysis. The association rules which fulfill the requirements will be visualized in the form of graphs, charts, or tables using their parameters such as lift, support, and confidence. By presenting the data visualization, changes of customer behavior could be observed based on the variation of the values in different time periods.

## Project Objectives
The following are the main objectives of this project:
1. **Conducting Market Basket Analysis in Different Time Periods**<br>
The project will conduct a more specific market basket analysis with an additional factor which is the time period. Afterward, an analysis of transaction data from different time periods such as quarters, months, and biweeks, will be carried out using market basket analysis. The project will be initiated by conducting a market basket analysis using a dataset consisting of transactions in different time periods to determine the association rules.
2. **Analysis of the Fluctuation of Customer Purchasing Behaviors with the Changes in Time**<br>
By comparing the statistics of association rules over different time periods, a deeper understanding and better insight into why and how customer behavior changes over time will be obtained. The parameters of the association rules such as confidence, lift, and support will be visualized in the forms of graphs, charts, or plots for the observation of which of the values change over time. The hypothesis could be proven by comparing the real transaction data to check if the fluctuation of the transaction frequency. Next, the variation of customer purchasing behaviors with the changes in time will be analysed to understand how they change and identify if any external factors that may be influencing these changes.
3. **Actionable Recommendations for Optimal Time Period for Market Basket Analysis**<br>
Product recommendations will be provided to the businesses, along with the justifications, for tracking and analyzing the changes in customer behavior more effectively. The insights gained from the project will benefit businesses to adjust their marketing strategies from time to time (e.g. product placement, and pricing of the products) after understanding how the association rules variate over time. Nevertheless, recommendations for cross-selling and up-selling could be made by considering the external factors affecting the fluctuation of customer purchasing behaviors such as the frequency of customer purchases, length of time between purchases, and the seasonality of product demand.

## Key Components
1. **Data Collection**<br>
Primarily, the data collection module will concentrate on acquiring raw transactional data from pertinent sources such as retail databases, e-commerce platforms, or loyalty programs. This phase encompasses data extraction, refinement, transformation, and potentially integrating data from diverse origins. This module guarantees the precision, uniformity, and structured formatting of transactional data, rendering it suitable for subsequent analytical phases. The dataset to be used is a transactional dataset which contains all the transactions occurring between December of 2010 and 2011 and the source of the dataset originated from a UK-based and registered non-store online retail. The dataset contains a total of 541909 rows and 9 columns which correspond to invoice number, stock code, description of product, quantity, invoice date, customer code, country, and biweek.
2. **Data Preprocessing**<br>
For the pre-processing steps, the â€˜InvoiceDateâ€™ column is converted to a datetime data type to perform further date-based operations. A new column â€˜Quarterâ€™ in the DataFrame is created by extracting the quarter information from the â€˜InvoiceDateâ€™ column. The code then calculates the number of transactions for each quarter and year combination by grouping the data according to â€˜Yearâ€™ and â€˜Quarterâ€™. Afterwards, the sum of transactions counts is calculated for each yearâ€™s four consecutive quarters and the top four quarters with the highest total transaction counts are selected. The code then filters the original Dataframe â€˜dfâ€™ to retain only the transactions from the selected four quarters and eventually the filtered DataFrame â€˜df_top_four_quartersâ€™ is grouped by the â€˜InvoiceNoâ€™, â€˜Yearâ€™, â€˜Quarterâ€™, and the â€˜Descriptionâ€™ column is aggregated into a list for each group. This results in a new DataFrame â€˜data_groupedâ€™ where each row represents a unique transaction. Consequently, an empty list named â€˜top_quarter_transactionsâ€™ is created to store the transactions. The code then iterates through the top four quarters which are sorted by their quarterly period and the â€˜data_groupedâ€™ DataFrame is filtered to extract only the transactions corresponding to that quarter for each quarter of the selected four quarters. The transactions for each quarter are assigned to separate variables for later processing operations purpose. Similarly, three consecutive months with the highest number of transactions are selected for the monthly MBA to discover the most significant association rules. Regarding the pre-processing of biweekly data, the transactional data for biweekly MBA is selected from the top three consecutive months with the highest cumulative transaction counts which were identified previously.
3. **One-hot Encoding**<br>
One-hot encoding is a popular technique used in data pre-processing to convert categorical variables into a binary representation that can be used by machine learning algorithms. A function â€˜perform_one_hot_encodingâ€™ which takes a list of transactions and an optional parameter â€˜remove_lowerâ€™ is defined. The function will remove columns containing any lowercase letters from the column names if â€˜remove_lowerâ€™ is set to True as these typically represent noise or insignificant items in the transaction data. The function converts the transaction list into a list of strings and initializes a â€˜TransactionEncoderâ€™ object from the â€˜mlxtend.preprocessingâ€™ module which is used to perform one-hot encoding on the transaction data. The output will be each column representing an item and each row represents a transaction where value of 1 indicates the presence of an item in a transaction and value of 0 indicates absence.
4. **Association Rule Mining**<br>
At the core of the project, the association rule mining module employs Apriori algorithm to extract association rules within each segmented time frame. The Apriori algorithm is a classic data mining technique used for association rule mining to discover interesting relationships between items in large datasets, particularly in Market Basket Analysis. This phase involves unearthing frequent itemsets and generating association rules based on predefined support and confidence thresholds. These processes unveil insights into item co-occurrence trends within transactions and uncover rules emblematic of purchasing behaviours specific to distinct periods. In the subsequent analysis and comparison module, the derived association rules from diverse time segments undergo assessment and juxtaposition. This module evaluates the weight, potency, and patterns of the unearthed rules. By contrasting rules across disparate temporal divisions, trends, and deviations within customer purchasing conduct are revealed. This evaluation assumes paramount importance in pinpointing the optimal period for efficient association rule mining.
5. **Visualization and Reporting**<br>
This module transforms the findings into comprehensible visual representations and reports. The generated reports provide a concise overview of the analysis results to facilitate decision-makers in gleaning insights and actionable recommendations.  A cross-comparison of rules across different months and biweekly periods is executed to pinpoint trends and shifts in customer preferences. Visual aids such as bar charts serve to effectively communicate patterns and trends embedded in the identified rules. The ensuing project employs succinct yet informative narratives to expound upon rule significance, and spotlight significant findings, temporal changes, and potential implications for business strategies.
6. **Interpretation and Recommendation**<br>
The findings are subsequently distilled into concise, actionable takeaways, summarizing complex insights. They synthesize pivotal patterns, trends, and variations identified through rule mining, accentuating their potential impact on business objectives. In this analysis, insightful recommendations are formulated and suggestions in terms of strategies for marketing campaigns, inventory management, or customer segmentation will be recommended. This summary streamlines comprehension of the analysis results and facilitates strategic decision-making guided by data-driven insights. This approach optimizes resource allocation, augments business performance, and enhances overall operational efficiency.

## Why Apriori Algorithm?
The Apriori algorithm is the first and most widely used data mining technique that was proposed by R. Agrawal and R. Srikant in 1994 for frequent itemset mining.  It is a bottom-up approach that uses a level-wise search for identifying the frequent item sets. There are two main steps in the Apriori algorithm which are the join step and the prune step. The join step generates (k+1)-itemsets from k-itemsets by joining each item with itself while the prune step determines the frequent itemsets that meet minimum support and disqualify those itemsets that are recognised as infrequent. In the first iteration of the algorithm, a 1-itemsets candidate is found by scanning the database in which the occurrence of the itemset is satisfying the minimum support threshold. Next, 2-itemsets that meet minimum support are determined using previous frequent 1-itemsets and this process will continue until frequent k-itemsets are found.
1. **Simplicity and Understandability**<br>
The Apriori algorithm is straightforward and easy to understand. It operates on a simple principle of finding frequent itemsets through multiple iterations. The algorithm first identifies individual items that meet a minimum support threshold. Then, it iteratively combines these items to form larger itemsets, again checking if they meet the support threshold. This process is repeated until no more frequent itemsets can be generated. This step-by-step approach is intuitive and makes the algorithm easy to implement and explain, even to those who may not be data science experts. Its simplicity also allows for clear communication of the results to stakeholders.
2. **Efficiency with Large Datasets**<br>
Apriori can handle large datasets effectively through its use of pruning and candidate generation. The bottom-up approach ensures that itemsets are expanded only when all of their subsets are frequent. This helps in reducing the number of candidate itemsets at each iteration. By doing so, it avoids unnecessary computations and focuses on potentially frequent itemsets, improving efficiency. The pruning step, where infrequent itemsets are eliminated early, further enhances performance, making Apriori suitable for large-scale market basket analysis.
3. **Breadth of Application**<br>
Apriori is widely applicable and can be used for a variety of association rule mining tasks across different domains. Because of its widespread use, there are many optimized versions and implementations of Apriori available, which can further enhance its performance. These optimizations include techniques such as hash-based itemset counting, transaction reduction, and partitioning, which make Apriori robust in handling a variety of real-world scenarios. Its reliability and the ease with which results can be interpreted make it a preferred choice for many market analysts.

## Getting Started
This guide will help you set up your development environment using Spyder IDE or any other cell-based notebook environment like Jupyter Notebook. Follow these steps to get started with analyzing transaction data and discovering valuable insights. 
### Prerequisites
Before you begin, ensure you have the following software installed on your machine:
* Anaconda Distribution (which includes Spyder IDE and Jupyter Notebook)
* Python 3.8 or higher

## Contributing
We appreciate your interest in contributing to the Market Basket Analysis project. Whether you are offering feedback, reporting issues, or proposing new features, your contributions are invaluable. Here's how you can get involved:
### How to Contribute
1. **Issue Reporting**
   * If you encounter any issues or unexpected behavior, please open an issue on the project.
   * Provide detailed information about the problem, including steps to reproduce it.
2. **Feature Requests**
   * Share your ideas for enhancements or new features by opening a feature request on GitHub.
   * Clearly articulate the rationale and potential benefits of the proposed feature.
3. **Pull Requests**
   * If you have a fix or an enhancement to contribute, submit a pull request.
   * Ensure your changes align with the project's coding standards and conventions.
   * Include a detailed description of your changes.
  
## License
The Market Basket Analysis project is open-source and licensed under the [MIT License](LISENCE). By contributing to this project, you agree that your contributions will be licensed under this license. Thank you for considering contributing to our project. Your involvement helps make this project better for everyone. <br><br>
**Have Fun!** ðŸš€
